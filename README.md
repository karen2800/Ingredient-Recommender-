# Ingredient Recommender System
This project focuses on recommending ingredients to add to or remove from a given recipe. It can help determine which ingredients are missing from a recipe or help alter a recipe to fit a person's needs. The recommended ingredients need to go well with the other ingredients in the recipe. I viewed this as a classification problem where ingredients are classified as either belonging to a recipe or not belonging to a recipe. Naïve Bayes, Non-negative Matrix Factorization (NMF), and k Nearest Neighbors (KNN) are compared. Naïve Bayes and KNN were altered to deal with the imbalance problem in the data, and the altered version of Naïve Bayes performed the best. 

The data comes from a 2017 archived dataset from AllRecipes. I reduced the number of recipes to only good recipes, where the star rating had to be at least 4 stars, and leaving recipes with rare ingredients. My data had two big problems. First, most ingredients do not belong in a recipe. This leads to a large imbalance problem, since it is usually better not to recommend an ingredient for a recipe. My second problem was having an imbalanced ingredient distribution. Only a small number of ingredients were extremely popular among most of the recipes while the vast majority were not. This included ingredients such as salt, flour, black pepper, and butter. Since the accuracy of always classifying negatively was 91%, pure accuracy was not very valuable in measuring accuracy. Instead I used the f1 score and the Matthews Correlation Coefficient (MCC), which are both better measures of accuracy for imbalanced data. 

### Modeling
**Naive Bayes**:
I calculated all of the co-occurrence counts between the ingredients. These co-occurrences were the number of recipes both ingredients were found together in. In the test phase, for each test recipe, I calculated the posteriors of all ingredients possible. Then, I sorted the ingredients by posterior. I chose to recommend the top ingredient, ignoring other ingredients already present in the recipe. If this top recommended ingredient was actually present in the recipe, then I would mark that recommendation as a true positive. Otherwise, it would be a false positive. 

I was concerned about the imbalance in my data and considering that Naïve Bayes assumes i.i.d. (independent and identically distributed), I made changes to my calculation of the posterior. I normalized the likelihoods by dividing them by the number of recipes that the ingredient in the given recipe was in. This helped balance the imbalanced data. I also did not want to always favor popular ingredients because their popularity does not mean they should always be recommended. This is why I decided to use a uniform prior.

**Non-negative Matrix Factorization**: This involves decomposing the recipe-ingredient matrix into two matrices, with the limitation of non-negativity in these matrices. This added constraint is beneficial because of the greater level of interpretability in understanding the recipe-ingredient interactions. I used the ML.NET framework to implement this, using stochastic gradient descent for optimization. The recommendations were the ingredients sorted by their prediction score. Evaluation was similar to how Naïve Bayes was evaluated, except instead of posterior probabilities, I had prediction scores.

**K Nearest Neighbors**. The goal in using k nearest neighbors was to have neighboring recipes vote whether an ingredient belongs or not in a recipe based on if the majority of neighbors contain that ingredient or not. In testing, for each test recipe I calculated the distances between the test recipe and all of the training recipes. I sorted the training recipes by distance and found the k nearest recipes. Then, for each possible ingredient, the k nearest recipes voted whether to recommend the ingredient for the test recipe or not. A neighboring recipe voted to recommend if it contained the ingredient. Otherwise, it voted to not recommend the ingredient. 

I tried three different distance measurements: Hamming distance, Levenshtein distance, and Jaccard distance. Hamming distance is the number of differences. Levenshtein distance is the minimum number of edits needed to make one recipe equal to the other. I used a dynamic programming approach to implement this to reduce the time complexity. Jaccard distance is 1 minus the Jaccard index. The Jaccard index is the intersection divided by the union. I used 5-fold cross validation to determine the optimal values of k for each of the distance measures, maximizing the f1 score.

**Modified k Nearest Neighbors**. I was concerned that I was not getting a good representation of all of the ingredients in my test recipe, so I made some changes to my implementation of KNN. I noticed that there were many cases where the nearest neighbors did not contain all or most of the ingredients in the test recipe. This could be because of the imbalance in my data. But having a constant value of k nearest neighbors made it difficult to make further improvements since my k nearest neighbors often did not have most of the test recipe ingredients, especially if these ingredients were not as common. 

To deal with the imbalance in my data, I decided to modify my KNN solution so that instead of having a constant value of k, I have a dynamic value. Once I have calculated the distances and sorted them, instead of just picking the top k nearest recipes and making a recommendation decision based on their votes, I start with a k value of 1, and increase it, adding one neighbor recipe at a time, until I have seen all of the test recipe ingredients in my k nearest neighbors. I also set the minimum k value equal to the optimal k value I had determined before for Jaccard. For each of the k nearest neighbors, I kept track of the number of times I saw an ingredient, except instead of adding 1 each time I saw an ingredient, I added the similarity score for the recipe it was in. I sorted the ingredients by their scores, which left me with the recommended ingredients for the given test recipe. I decided to use Jaccard similarity since Jaccard distance showed good results in my previous method.  I measured accuracy by taking the top recommended ingredient, ignoring other ingredients already present in the recipe, and seeing if the top recommended ingredient belonged in the recipe or not. 

### Conclusions
Overall, I was able to recommend ingredients to recipes. Naïve Bayes performed the best when using normalization, laplace smoothing, and uniform prior. Normalization, in particular, made the biggest difference in the increase in f1 scores. This means that normalization was able to effectively deal with the imbalance problem. My second-best model was Modified k Nearest Neighbors, which attempted to deal with the popularity of ingredients problem. Using a dynamic k and increasing ingredient scores by Jaccard similarity for each neighboring recipe containing it helped deal with this ingredient distribution imbalance. Understanding the data was very important because once I addressed the imbalance problem, the accuracy of my models increased significantly. 

